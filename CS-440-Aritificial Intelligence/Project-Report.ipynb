{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Gomoku Game - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Jim Xu, Dec. 2017*\n",
    "\n",
    "[jimx@rams.colostate.edu](mailto://jimx@rams.colostate.edu)\n",
    "\n",
    "### Reinforcement Learning for Gomoku Game - [Source](https://nbviewer.jupyter.org/github/GrEedWish/csu-cs-440/blob/master/Project-Source.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artifical Intelligence plays an important role in today's game development. I am interested in what is the AI robots training process and how accurate the trained robots behave in the games. Many popular games, for example, DOTA2, CS and LOL, have their own AI system creating computer-controlled robots. The robot is trained to make the optimal decision under the messy, complicated situations. The introducion of AI system remarkably improves the playability and enjoyability of the game, more and more emphasis has been addressed on AI development in Game Industry. More importantly, a breaking news [(a bot which beats the world’s top professionals at 1v1 matches of Dota 2)](https://blog.openai.com/dota-2/) shows that how powerful AI system could be, and it greatly provokes my enthusiasm to learning AI for game development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project will focus on the problem called [Gomoku game](https://en.wikipedia.org/wiki/Gomoku), The idea is derived from a simpler classical problem [tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe) game. The rule of Gomoku game is two players alternating turns placing a piece of their color on an empty intersection. The winner is the first player to form an unbroken chain of five stones horizontally, vertically, or diagonally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to train a computer-controlled robot, who initially has no knowledge about the rules and strategies, with the ability to make the \"best\" moves leading to a win in the game. **Reinforcement\n",
    "Learning (RL)** algorithms are applied to find optimal policy that maximizes the winning possibility for the computer-controlled robot, based on the notebook [Reinforcement Learning for Two-Player Games](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/15%20Reinforcement%20Learning%20for%20Two-Player%20Games.ipynb). The computer-controlled robot initially has no knowledge of the Gomoku game rules, while its opponent has some relative experience and rules of Gomoku game. Reinforcement Learning is used to train a robot AI player which can quickly learn the rules of Gomoku. The model accuracy is tested by calculating the win percentage for the well-trained AI robot. To make the problem more doable, this project will constraint on a 5x5 board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Picture 1](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is the main method to tackle with the Gomoku game problem. Which may include:\n",
    "\n",
    "1) Generate local-optimal moves for a given player\n",
    "   + [Negamax](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/12%20Negamax.ipynb) is used to find a maximized node value for the given player. \n",
    "   + [Alpha–beta pruning](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/12%20Negamax.ipynb) is used to cut down the number of nodes that are evaluated when performing Negamax.\n",
    "   + [Iterative Deepening](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/05%20Iterative%20Deepening%20and%20Other%20Uninformed%20Search%20Methods.ipynb) is used to set a time limit of the algorithm spent to find a optimal move for the given player.\n",
    "   \n",
    "  \n",
    "2) Find optimal policy for the agent\n",
    "   + [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process) is used to find policy that maximizes cumulative reward(score for win) for the robot at a given environment(state).\n",
    "   + [Reinforcement Learning](http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/15%20Reinforcement%20Learning%20for%20Two-Player%20Games.ipynb)(RL) is used to calculate Q-table for all possibilities (state and action).\n",
    "\n",
    "The game will start by assuming AI has no knowledge of Gomoku game rules, and chooses pseudo-random moves (pseudo-random: instead of trying each intersections on broad, pseudo-random will constrain the random nodes to a limited, calculable number on broad). The opponent with some background knowledge will choose local-optimal move calculated by iterative deepening negamax algorithm. After considerable times of reinforcement learning iterations, the robot will transfer into Q-value orientated move choice, while its opponent still using local-optimal moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *More details in Project Source*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Calculate score for a given move on board using **some(partial)** real world knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalty: (initial value for a given move based on its position) calculated by its deviation from board center. For a given move at position (x', y'), considering the center(x, y), the penalty is:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(move)   = -2 \\times ( (x-x')^2 + (y-y')^2 )\n",
    "\\end{equation*}\n",
    "\n",
    "Basics: Calculating scores in four different directions and get the max of them. (vertical, horizontal and diagonals)\n",
    "\n",
    "Scores: +1 when encounter empty cell in a given direction, +2 when encounter cell with same player type in a given direction\n",
    "\n",
    "Bonus:\n",
    "+ diagonal: +1 when in diagonal direction\n",
    "+ consecutive:\n",
    "    + $ +  1 \\times (n - 1)$ when encounter n consecutives empty cells\n",
    "    + $ +  3 \\times (n - 1)$ when encounter n consecutives identical player cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initally, the table looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Table 1](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) get optimal moves based on the currect board scores for a given palyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Type: return the move with highest score in the board, if many, take any.\n",
    "\n",
    "Second Type: return # of moves with top # highest scores in the borad, # equals the minimum of user specific number and the number of moves available.\n",
    "\n",
    "Third Type: return a random move from Second Type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Negamax with Alpha–beta pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm optimizations for minimax are also equally applicable for Negamax. Alpha-beta pruning can decrease the number of nodes the negamax algorithm evaluates in a search tree in a manner similar with its use with the minimax algorithm. (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Which is the optimal algorithm based on current score scheme? <a id='comparsion'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. local optimal (negamaxabIDS) algorithm\n",
    "2. move with highest score in the board\n",
    "3. random move in top # highest moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Test Iterations | Type 1 Win Rate| Type 2 Win Rate | Type 3 Win Rate |\n",
    "|-----------------|--------|--------|--------|\n",
    "| 100 | 1.0 | 0.99 | 0.88 |\n",
    "| 500 | 0.996 | 0.996 | 0.864 |\n",
    "| 1000 | 0.992 | 0.993 | 0.871 |\n",
    "| 5000 | 0.9938 | 0.9934 | 0.8774 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As table shown, local optimal (negamaxabIDS) algorithm and move with highest score in the board yields similar win rate, while they both suffer from moves following a fixed pattern weakness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) What are the pros and cons of different algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three algorithms yield desirable win rate but there are some problems with this algorithm.\n",
    "\n",
    "Back to the board score calculation function, the criterion for score only examinate the score it earned for taking that move when comparing with empty cells or identical cells. **It does not take its opponent board status into account.** It does not receive penalty for oppoent's potential threats or winning, and hence it is not a good score calculation standard for real Gomoku Game.\n",
    "\n",
    "Local optimal (negamaxabIDS) algorithm and move with highest score in the board suffer from moves following a fixed pattern weakness. According to a fixed score calculation equation, opponent is likely to take exactly same steps leading to his potential win, while the training agent could easier exploit a silly stratage to claim its win against its opponent, see picture below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Picture 2](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, black (opponent) will follow his diagonal track since those moves are the highest score for a winning stage for itself. However, if white moves first, it could use a silly stratage (arrange its pieces into consecutive 4 in a horizontal line) for winning before black could achieve consecutive 4 in diagonal.\n",
    "\n",
    "There are two ways to mitigate this problem:\n",
    "\n",
    "First, let negamaxabIDS moves first, then training agent will develop a pattern to defend IDS score scheme and achieve its own win at the same time (defensive and offensive model).\n",
    "\n",
    "Second, let opponent take random best move from a pool of top number of best moves. This will somehow mitigate the fixed pattern problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Which player moves first? Training model or Opponent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model taking random move first will develops an offensive model for training agent, Opponent taking move based on real-world stratage first develops an defensive model for training agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Should  negamaxabIDS be used in training process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, negamaxabIDS will provide the best trainning results among the three candidates. However, the way I calculated the scores determined negamaxabIDS will always find moves that lead to consecutive pieces of its own, without considering its opponents move (**negamaxabIDS always leads to offensive move**). It is highly likely that the trained agent model has well performance under my score calculation scheme while failed on other more accurate score calculation scheme, when combining offensive and defensive stratage. To sum up, it is not suitable to use negamaxabIDS in training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Using Reinforcement Learning Mechanism to train a model against optimal move stratage with agent moves first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![P3](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training process is very successful since the mean of winning rate for training agent is approximately 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initally, the trained probability for taking a move looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![P4](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here raises a question, why does the trained model prefer top-left over middle part? Here shows how the trained model defeats the opponent using NegamaxabIDS score scheme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![P6](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is the score scheme I proposed perfers middle region over periphery region, and conversely the trained model expolit the weakness of this pattern and place piece in periphery region to gain a win. **Although the training process was successful, the trained model is silly and not useful at all.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Using Reinforcement Learning Mechanism to train a model against optimal move stratage with oppoent moves first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![P7](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training process is very successful since the mean of winning rate for training agent is approximately 1.\n",
    "\n",
    "**This trained model suffers from the same problem, the opponent will not consider agent's move while searching its own highest score. **  Here shows how the trained model defeats the opponent using NegamaxabIDS score scheme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![P8](https://raw.githubusercontent.com/GrEedWish/csu-cs-440/master/Picture8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent did develop a pattern to defend from opponent getting win at the third step. However, the opponent does not defend agent in the second last move due to the weakness of score scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Using Reinforcement Learning Mechanism to train a model against random move stratage with oppoent moves first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful check is using Reinforcement Learning Q Table trained by NegamaxabIDS algorithm, playing against random move startage opponent to calculate the winning rate. [Refer to above](#comparsion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. local optimal (negamaxabIDS) algorithm\n",
    "2. move with highest score in the board\n",
    "3. random move in top # highest moves\n",
    "4. Q-Table orientated algorithm\n",
    "5. random Stratage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Test Iterations | Type 1 Win Rate| Type 2 Win Rate | Type 3 Win Rate | Q-Table Win Rate | Random Win Rate |\n",
    "|-----------------|--------|--------|--------|--------|--------|\n",
    "| 100 | 1.0 | 0.99 | 0.88 | 0.83 |  0.42 |\n",
    "| 500 | 0.996 | 0.996 | 0.864 | 0.834 | 0.382 |\n",
    "| 1000 | 0.992 | 0.993 | 0.871 | 0.823 | 0.393 |\n",
    "| 5000 | 0.9938 | 0.9934 | 0.8774 | 0.8184 | 0.4094 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be concluded from the above comparsion is that the trained model did learn some rules from Reinforcement Learning Mechanism (Although it has silly performance against a specific stratage) and achieved relative high winning rate (around 0.8) comparing with no knowledge orientated (around 0.4) (random v.s. random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The training process is successful to build a model playing against opponent using best move according to current score scheme. The trained model has gain some knowledge on how to play Gomuku game while the trained model is not good enough for general cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Develop a better score calculation standard: Both offensive and defensive\n",
    "2. Integrate Neural Network as Q function into Reinforcement learning\n",
    "3. Extending the game to a larger scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this project, I have a thorough understanding of Negamax with Alpha-Beta Pruning Iterative Deepening Algorithm, especially for multi-agents. During the exploration of a correct pattern for multi-agents, I tried to specify the agent in each round and return last score in a decent manner (positive or negative, taking min or max). Besides, Negamax with Alpha-Beta Prunning also becomes me a favorable algorithm when counting reward for an action and agent, which is common and super useful in game system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, this project makes me reflects a lot on the Reinforcement Learning mechanism, including 1) how to implement 2-agents RL learning process 2) how to choose opponent stratage to train a reasonable and convincing model 3) how to keeping track of intermediate data and visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized I did not do a perfect job meeting my original requirements due to some unexpected impediments half-way through. However, I did try, and do my best to resolve those new problems and reflect on potential improvments on my current work frame. Overall, this project experience is really wonderful and of great significance in guiding my future work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
