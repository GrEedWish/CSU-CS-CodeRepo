{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}\\;}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}\\;}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when we were discussing the complexity of models?  The simplest model was a constant.  A simple way to predict rainfall is to ignore all measurements and just predict the average rainfall.  If a linear model of measurements may do no better.  The degree to which it does do better can be expressed in the relative sum of squared errors (RSE) or\n",
    "\n",
    "$$RSE = \\frac{\\sum_{i=1}^N (\\tv_i - \\xv_i^T \\wv)^2}{\\sum_{i=1}^N (\\tv_i - \\bar{\\Tv})^2}$$\n",
    "\n",
    "If RSE is 1, then your linear model is no better than using the mean.  The closer RSE is to 0, the better your linear model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O http://mlr.cs.umass.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n",
    "!curl -O http://mlr.cs.umass.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMPGData(filename='auto-mpg.data'):\n",
    "    def missingIsNan(s):\n",
    "        return np.nan if s == b'?' else float(s)\n",
    "    data = np.loadtxt(filename, usecols=range(8), converters={3: missingIsNan})\n",
    "    print(\"Read\",data.shape[0],\"rows and\",data.shape[1],\"columns from\",filename)\n",
    "    goodRowsMask = np.isnan(data).sum(axis=1) == 0\n",
    "    data = data[goodRowsMask,:]\n",
    "    print(\"After removing rows containing question marks, data has\",data.shape[0],\"rows and\",data.shape[1],\"columns.\")\n",
    "    X = data[:,1:]\n",
    "    T = data[:,0:1]\n",
    "    Xnames =  ['bias', 'cylinders','displacement','horsepower','weight','acceleration','year','origin']\n",
    "    Tname = 'mpg'\n",
    "    return X,T,Xnames,Tname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,T,Xnames,Tname = makeMPGData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X.mean(0)\n",
    "stds = X.std(0)\n",
    "nRows = X.shape[0]\n",
    "Xs1 = np.insert((X-means)/stds, 0, 1, axis=1) # insert column of ones in new 0th column\n",
    "# Xs1 = np.hstack(( np.ones((nRows,1)), (X-means)/stds ))\n",
    "w = np.linalg.lstsq(Xs1.T @ Xs1, Xs1.T @ T)[0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict = Xs1.dot(w)\n",
    "predict = Xs1 @ w\n",
    "RSE = np.sum((T-predict)**2) / np.sum((T - T.mean(0))**2)\n",
    "RSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our linear model seems to be quite a bit better than using just the mean mpg.  If our linear model is worse than using the mean, this value would be greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, maybe our model would do better if it was a little closer to just the mean, or, equivalently, just using the bias weight.  This is the question that drives the derivation of \"ridge regression\".  Let's add a term to our sum of squared error objective function that is the sum of all weight magnitudes except the bias weight.  Then, we not only minimize the sum of squared errors, we also minimize the sum of the weight magnitudes:\n",
    "\n",
    "$$ \\sum_{i=1}^N (\\tv_i - \\xv_i^T \\wv)^2 + \\lambda \\sum_{i=2}^N w_i^2$$\n",
    "\n",
    "Notice that $\\lambda$ in there.  With $\\lambda=0$ we have our usual linear regression objective function. With $\\lambda>0$, we are adding in a penalty for the weight magnitudes.\n",
    "\n",
    "How does this change our solution for the best $\\wv$?  You work it out.  You should get\n",
    "\n",
    "$$ \\wv = (X^T X + \\lambda I)^{-1} X^T T $$\n",
    "\n",
    "except instead of using \n",
    "\n",
    "$$\n",
    "    \\lambda I =\n",
    "     \\begin{bmatrix}\n",
    "     \\lambda & 0 & \\dotsc & 0\\\\\n",
    "       0 & \\lambda & \\dotsc & 0\\\\\n",
    "       \\vdots \\\\\n",
    "       0 & 0 & \\dotsc & \\lambda\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we want\n",
    "\n",
    "$$\n",
    "     \\begin{bmatrix}\n",
    "      0 & 0 & \\dotsc & 0\\\\\n",
    "       0 & \\lambda & \\dotsc & 0\\\\\n",
    "       \\vdots \\\\\n",
    "       0 & 0 & \\dotsc & \\lambda\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "so we don't penalize the bias weight, the weight multiplying the constant 1 input component.\n",
    "\n",
    "\n",
    "Now, what value should $\\lambda$ be?  Must determine empirically, by calculating the sum of squared error on test data.\n",
    "\n",
    "Actually, we should not find the best value of $\\lambda$ by comparing error on the test data. This will give us a too optimistic prediction of error on novel data, because the test data was used to pick the best $\\lambda$.  We really must hold out another partition of data from the training set for this. This third partition is often called the model validation set.  So, we partition our data into disjoint training, validation, and testing subsets, and\n",
    "\n",
    "* For each value of $\\lambda$\n",
    "  * Solve for $\\wv$ using the training set\n",
    "  * Use $\\wv$ to predict the output for the validation set and calculate the squared error.\n",
    "  * Use $\\wv$ to predict the output for the testing set and calculate the squared error.\n",
    "  * Pick value of $\\lambda$ that produced the lowest validation set error, and report the testing set error obtained using that value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.1\n",
    "D = Xs1.shape[1]\n",
    "lambdaDiag = np.eye(D) * lamb\n",
    "lambdaDiag[0,0] = 0\n",
    "lambdaDiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLambda(D,lamb=0):\n",
    "    lambdaDiag = np.eye(D) * lamb\n",
    "    lambdaDiag[0,0] = 0\n",
    "    return lambdaDiag\n",
    "makeLambda(3,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.lstsq(Xs1.T @ Xs1 + lambdaDiag, Xs1.T @ T)[0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Xs1.shape[1]\n",
    "wLambda10 = np.linalg.lstsq(Xs1.T @ Xs1 + makeLambda(D,10.0), Xs1.T @ T)[0]\n",
    "wLambda0 = np.linalg.lstsq(Xs1.T @ Xs1 + makeLambda(D,0), Xs1.T @ T)[0]\n",
    "np.hstack((wLambda10,wLambda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0,0.1,1,10,100]\n",
    "for lamb in lambdas:\n",
    "    w = np.linalg.lstsq(Xs1.T @ Xs1 + makeLambda(D,lamb), Xs1.T @ T)[0]\n",
    "    plt.plot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0,0.1,1,10,100]\n",
    "for lamb in lambdas:\n",
    "    w = np.linalg.lstsq(Xs1.T @ Xs1 + makeLambda(D,lamb), Xs1.T @ T)[0]\n",
    "    plt.plot(w, '-o')\n",
    "plt.xticks(range(8), Xnames, rotation=30, horizontalalignment='right')\n",
    "plt.ylabel('$\\mathbf{w}$', fontsize='large', rotation='horizontal', labelpad=20)\n",
    "plt.legend(lambdas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are not modifying the bias weight, let's take it out of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0,0.1,1,10,100]\n",
    "for lamb in lambdas:\n",
    "    w = np.linalg.lstsq(Xs1.T @ Xs1 + makeLambda(D,lamb), Xs1.T @ T)[0]\n",
    "    plt.plot(w[1:], '-o')\n",
    "plt.xticks(range(7), Xnames[1:], rotation=30, horizontalalignment='right')\n",
    "plt.ylabel('$\\mathbf{w}$', fontsize='large', rotation='horizontal', labelpad=20)\n",
    "plt.legend(lambdas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "lambdas = [0, 10, 100, 1000, 10000]\n",
    "for lamb in lambdas:\n",
    "    w = np.linalg.lstsq(Xs1.T @ Xs1 + makeLambda(D,lamb), Xs1.T @ T)[0]\n",
    "    plt.plot(Xs1[:30] @ w)\n",
    "plt.plot(T[:30],'ro',lw=5,alpha=0.8)\n",
    "plt.ylabel('Model Output')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.legend(lambdas,loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which $\\lambda$ value is best?  Careful.  What is the best value of $\\lambda$ if just comparing error on training data?\n",
    "\n",
    "Now, careful again!  Can't report expected error from testing data that is also used to pick best value of $\\lambda$.  Error is likely to be better than what you will get on new data that was not used to train the model and also was not used to pick value of $\\lambda$.\n",
    "\n",
    "Need a way to partition our data into training, validation and testing subsets.  Let's write a function that makes these partitions randomly, given the data matrix and the fractions to be used in the three partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(X,T,trainFraction=0.8, validateFraction=0.1, testFraction=0.1):\n",
    "    '''Usage: Xtrain,Ttrain,Xval,Tval,Xtest,Ttext = partition(X,T,0.8,0.2,0.2)'''\n",
    "    if trainFraction + validateFraction + testFraction != 1:\n",
    "        raise ValueError(\"Train, validate and test fractions must sum to 1. Given values sum to \" + str(trainFraction+validateFraction+testFraction))\n",
    "    n = X.shape[0]\n",
    "    nTrain = round(trainFraction * n)\n",
    "    nValidate = round(validateFraction * n)\n",
    "    nTest = round(testFraction * n)\n",
    "    if nTrain + nValidate + nTest != n:\n",
    "        nTest = n - nTrain - nValidate\n",
    "    # Random order of data matrix row indices\n",
    "    rowIndices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(rowIndices)\n",
    "    # Build X and T matrices by selecting corresponding rows for each partition\n",
    "    Xtrain = X[rowIndices[:nTrain],:]\n",
    "    Ttrain = T[rowIndices[:nTrain],:]\n",
    "    Xvalidate = X[rowIndices[nTrain:nTrain+nValidate],:]\n",
    "    Tvalidate = T[rowIndices[nTrain:nTrain+nValidate],:]\n",
    "    Xtest = X[rowIndices[nTrain+nValidate:nTrain+nValidate+nTest],:]\n",
    "    Ttest = T[rowIndices[nTrain+nValidate:nTrain+nValidate+nTest],:]\n",
    "    return Xtrain,Ttrain,Xvalidate,Tvalidate,Xtest,Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(20).reshape((10,2))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.arange(10).reshape((-1,1))\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.arange(20).reshape((10,2)) + np.random.uniform(0,1,(10,2))\n",
    "T = np.arange(10).reshape((-1,1))\n",
    "Xtrain,Ttrain,Xval,Tval,Xtest,Ttest = partition(X,T,0.6,0.2,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\")\n",
    "print(Xtrain)\n",
    "print(\" Ttrain:\")\n",
    "print(Ttrain)\n",
    "print(\"\\n Xval:\")\n",
    "print(Xval)\n",
    "print(\" Tval:\")\n",
    "print(Tval)\n",
    "print(\"\\n Xtest:\")\n",
    "print(Xtest)\n",
    "print(\" Ttest:\")\n",
    "print(Ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model using several values of $\\lambda$ on Xtrain,Train and calculate the model error on Xval,Tval. Then pick best value of $\\lambda$ based on error on Xval,Tval. Finally, calculate error of model using best $\\lambda$ on Xtest,Ttest as our estimate of error on new data.\n",
    "\n",
    "However, these errors will be affected by the random partitioning of the data.  Repeating with new partitions may result in a different $\\lambda$ being best.  We should repeat the above procedure many times and average over the results.  How many repetitions do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach, commonly followed in machine learning, is to first partition the data into $k$ subsets, called \"folds\".  Pick one fold to be the test partition, another fold to be the validate partition, and collect the remaining folds to be the train partition.  We can do this $k\\,(k-1)$ ways. (Why?)  So, with $k=5$ we get 20 repetitions performed in a way that most distributes samples among the partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a little note on the `yield` statement in python. The `yield` statement is like `return` except that execution pauses at this point in the function, after returning the values.  When the function is called again, it continues from that paused point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(n):\n",
    "    for a in range(n):\n",
    "        yield a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in count(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def partitionKFolds(X,T,nFolds,shuffle=False,nPartitions=3):\n",
    "    '''Usage: for Xtrain,Ttrain,Xval,Tval,Xtest,Ttext in partitionKFolds(X,T,5,shuffle=True,nPartitions=3):\n",
    "            for Xtrain,Ttrain,Xtest,Ttext in partitionKFolds(X,T,5,shuffle=True,nPartitions=2):'''\n",
    "    # Randomly arrange row indices\n",
    "    rowIndices = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(rowIndices)\n",
    "    # Calculate number of samples in each of the nFolds folds\n",
    "    nSamples = X.shape[0]\n",
    "    nEach = int(nSamples / nFolds)\n",
    "    if nEach == 0:\n",
    "        raise ValueError(\"partitionKFolds: Number of samples in each fold is 0.\")\n",
    "    # Calculate the starting and stopping row index for each fold.\n",
    "    # Store in startsStops as list of (start,stop) pairs\n",
    "    starts = np.arange(0,nEach*nFolds,nEach)\n",
    "    stops = starts + nEach\n",
    "    stops[-1] = nSamples\n",
    "    startsStops = list(zip(starts,stops))\n",
    "    # Repeat with testFold taking each single fold, one at a time\n",
    "    for testFold in range(nFolds):\n",
    "        if nPartitions == 3:\n",
    "            # Repeat with validateFold taking each single fold, except for the testFold\n",
    "            for validateFold in range(nFolds):\n",
    "                if testFold == validateFold:\n",
    "                    continue\n",
    "                # trainFolds are all remaining folds, after selecting test and validate folds\n",
    "                trainFolds = np.setdiff1d(range(nFolds), [testFold,validateFold])\n",
    "                # Construct Xtrain and Ttrain by collecting rows for all trainFolds\n",
    "                rows = []\n",
    "                for tf in trainFolds:\n",
    "                    a,b = startsStops[tf]                \n",
    "                    rows += rowIndices[a:b].tolist()\n",
    "                Xtrain = X[rows,:]\n",
    "                Ttrain = T[rows,:]\n",
    "                # Construct Xvalidate and Tvalidate\n",
    "                a,b = startsStops[validateFold]\n",
    "                rows = rowIndices[a:b]\n",
    "                Xvalidate = X[rows,:]\n",
    "                Tvalidate = T[rows,:]\n",
    "                # Construct Xtest and Ttest\n",
    "                a,b = startsStops[testFold]\n",
    "                rows = rowIndices[a:b]\n",
    "                Xtest = X[rows,:]\n",
    "                Ttest = T[rows,:]\n",
    "                # Return partition matrices, then suspend until called again.\n",
    "                yield Xtrain,Ttrain,Xvalidate,Tvalidate,Xtest,Ttest,testFold\n",
    "        else:\n",
    "            # trainFolds are all remaining folds, after selecting test and validate folds\n",
    "            trainFolds = np.setdiff1d(range(nFolds), [testFold])\n",
    "            # Construct Xtrain and Ttrain by collecting rows for all trainFolds\n",
    "            rows = []\n",
    "            for tf in trainFolds:\n",
    "                a,b = startsStops[tf]                \n",
    "                rows += rowIndices[a:b].tolist()\n",
    "            Xtrain = X[rows,:]\n",
    "            Ttrain = T[rows,:]\n",
    "            # Construct Xtest and Ttest\n",
    "            a,b = startsStops[testFold]\n",
    "            rows = rowIndices[a:b]\n",
    "            Xtest = X[rows,:]\n",
    "            Ttest = T[rows,:]\n",
    "            # Return partition matrices, then suspend until called again.\n",
    "            yield Xtrain,Ttrain,Xtest,Ttest,testFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(20).reshape((10,2))\n",
    "T = np.arange(10).reshape((-1,1)) + np.random.uniform(-1, 1, (10,1))\n",
    "k = 0\n",
    "for Xtrain,Ttrain,Xval,Tval,Xtest,Ttest,testFold in partitionKFolds(X,T,5):\n",
    "    k += 1\n",
    "    print(\"Fold\",k)\n",
    "    print(\" Xtrain:\")\n",
    "    print(Xtrain)\n",
    "    print(\" Ttrain:\")\n",
    "    print(Ttrain)\n",
    "    print(\"\\n Xval:\")\n",
    "    print(Xval)\n",
    "    print(\" Tval:\")\n",
    "    print(Tval)\n",
    "    print(\"\\n Xtest:\")\n",
    "    print(Xtest)\n",
    "    print(\" Ttest:\")\n",
    "    print(Ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical use of these partitions is shown below.  It is most handy to just collect all results in a matrix and calculate averages afterwards, rather than accumulating each result and dividing by the number of repetitions at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, T, lamb):\n",
    "    means = X.mean(0)\n",
    "    stds = X.std(0)\n",
    "    n,d = X.shape\n",
    "    Xs1 = np.insert( (X - means)/stds, 0, 1, axis=1)\n",
    "    lambDiag = np.eye(d+1) * lamb\n",
    "    lambDiag[0,0] = 0\n",
    "    w = np.linalg.lstsq( Xs1.T @ Xs1 + lambDiag, Xs1.T @ T)[0]\n",
    "    return {'w': w, 'means': means, 'stds': stds}\n",
    "\n",
    "def use(model, X):\n",
    "    Xs1 = np.insert((X-model['means'])/model['stds'], 0, 1, axis=1)\n",
    "    return Xs1 @ model['w']\n",
    "\n",
    "def rmse(A,B):\n",
    "    return np.sqrt(np.mean( (A-B)**2 ))\n",
    "\n",
    "def evaluate(model, X, T):\n",
    "    return rmse(use(model, X), T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 5, 20)\n",
    "results = []\n",
    "for Xtrain, Ttrain, Xval, Tval, Xtest, Ttest,_ in partitionKFolds(X, T, 5):\n",
    "    for lamb in lambdas:\n",
    "        model = train(Xtrain, Ttrain, lamb)\n",
    "        predict = use(model, Xval)\n",
    "        results.append([lamb,\n",
    "                        evaluate(model, Xtrain, Ttrain),\n",
    "                        evaluate(model, Xval, Tval),\n",
    "                        evaluate(model, Xtest, Ttest)])\n",
    "results = np.array(results)\n",
    "# print(results)\n",
    "avgresults = []\n",
    "for lam in lambdas:\n",
    "    print(lam)\n",
    "    print(results[results[:,0]==lam,1:])\n",
    "    avgresults.append( [lam] + np.mean(results[results[:,0]==lam,1:],axis=0).tolist())\n",
    "avgresults = np.array(avgresults)\n",
    "print(avgresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avgresults[:,0],avgresults[:,[1, 3]],'o-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(('Train', 'Test'),loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is yet another python implementation that includes many of the common steps used when training and evaluating models.  We will use this version for many of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainValidateTestKFolds(trainf, evaluatef, X, T, parameterSets, nFolds=5,\n",
    "                            shuffle=False, verbose=False):\n",
    "    # Randomly arrange row indices\n",
    "    rowIndices = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(rowIndices)\n",
    "    isNewBetterThanOld = lambda new,old: new < old\n",
    "    # Calculate number of samples in each of the nFolds folds\n",
    "    nSamples = X.shape[0]\n",
    "    nEach = int(nSamples / nFolds)\n",
    "    if nEach == 0:\n",
    "        raise ValueError(\"trainValidateTestKFolds: Number of samples in each fold is 0.\")\n",
    "    # Calculate the starting and stopping row index for each fold.\n",
    "    # Store in startsStops as list of (start,stop) pairs\n",
    "    starts = np.arange(0,nEach*nFolds, nEach)\n",
    "    stops = starts + nEach\n",
    "    stops[-1] = nSamples\n",
    "    startsStops = list(zip(starts, stops))\n",
    "        \n",
    "    # Repeat with testFold taking each single fold, one at a time\n",
    "    results = []\n",
    "    for testFold in range(nFolds):\n",
    "        # Leaving the testFold out, for each validate fold, train on remaining\n",
    "        # folds and evaluate on validate fold. Collect theseRepeat with validate\n",
    "        # Construct Xtest and Ttest\n",
    "        a,b = startsStops[testFold]\n",
    "        rows = rowIndices[a:b]\n",
    "        Xtest = X[rows, :]\n",
    "        Ttest = T[rows, :]\n",
    "\n",
    "        bestParms = None\n",
    "        for parms in parameterSets:\n",
    "            # trainEvaluationSum = 0\n",
    "            validateEvaluationSum = 0\n",
    "            for validateFold in range(nFolds):\n",
    "                if testFold == validateFold:\n",
    "                    continue\n",
    "                # Construct Xtrain and Ttrain\n",
    "                trainFolds = np.setdiff1d(range(nFolds), [testFold, validateFold])\n",
    "                rows = []\n",
    "                for tf in trainFolds:\n",
    "                    a,b = startsStops[tf]                \n",
    "                    rows += rowIndices[a:b].tolist()\n",
    "                Xtrain = X[rows, :]\n",
    "                Ttrain = T[rows, :]\n",
    "                # Construct Xvalidate and Tvalidate\n",
    "                a,b = startsStops[validateFold]\n",
    "                rows = rowIndices[a:b]\n",
    "                Xvalidate = X[rows, :]\n",
    "                Tvalidate = T[rows, :]\n",
    "\n",
    "                model = trainf(Xtrain, Ttrain, parms)\n",
    "                # trainEvaluationSum += evaluatef(model,Xtrain,Train)\n",
    "                validateEvaluationSum += evaluatef(model, Xvalidate, Tvalidate)\n",
    "            validateEvaluation = validateEvaluationSum / (nFolds-1)\n",
    "            if verbose:\n",
    "                if hasattr(model, 'bestIteration') and model.bestIteration is not None:\n",
    "                    print('{} Val {:.3f} Best Iter {:d}'.format(parms, validateEvaluation, model.bestIteration))\n",
    "                else:\n",
    "                    print('{} Val {:.3f}'.format(parms, validateEvaluation))\n",
    "\n",
    "            if bestParms is None or isNewBetterThanOld(validateEvaluation, bestValidationEvaluation): \n",
    "                bestParms = parms\n",
    "                bestValidationEvaluation = validateEvaluation\n",
    "                if verbose:\n",
    "                    print('New best')\n",
    "                # trainEvaluation = trainEvaluationSum / (nFolds-1)\n",
    "\n",
    "        newXtrain = np.vstack((Xtrain, Xvalidate))\n",
    "        newTtrain = np.vstack((Ttrain, Tvalidate))\n",
    "\n",
    "        model = trainf(newXtrain, newTtrain, bestParms)\n",
    "        trainEvaluation = evaluatef(model, newXtrain, newTtrain)\n",
    "        testEvaluation = evaluatef(model, Xtest, Ttest)\n",
    "\n",
    "        # resultThisTestFold = [bestParms, trainEvaluation,\n",
    "        #                       bestValidationEvaluation, testEvaluation]\n",
    "        resultThisTestFold = [nFolds, testFold+1, bestParms, trainEvaluation, bestValidationEvaluation, testEvaluation]\n",
    "        results.append(resultThisTestFold)\n",
    "        if verbose:\n",
    "            print(resultThisTestFold)\n",
    "        print('{}/{}'.format(testFold+1, nFolds), end=' ', flush=True)\n",
    "    return np.array(results) # pandas.DataFrame(results, columns=('nFolds','testFold','parms','trainAcc','testAcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 5, 20)\n",
    "results = trainValidateTestKFolds(train, evaluate, X, T, lambdas, nFolds=5, \n",
    "                                  shuffle=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(results[:,1], results[:,3:6],'o-')\n",
    "plt.xticks(range(1,6))  # without this, we see 1.5, 2.5, etc.\n",
    "plt.xlabel('Fold Index')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(('Train','Validate','Test'),loc='best')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(results[:,1], results[:,2], '-o')\n",
    "plt.xticks(range(1,6))\n",
    "plt.xlabel('Fold Index')\n",
    "plt.ylabel('Best $\\lambda$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
